{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almosttomorrow/LLMbuilder/blob/main/My_Foundational_LLM_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "cK3W6vVeRS0c"
      },
      "outputs": [],
      "source": [
        "pip install -q streamlit torch transformers matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "CI7I9p6xSgh9"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "l7VMaMy1RZJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7ddb52-e053-4cc4-a859-abfcb9992e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the transformer model with encoder and decoder\n",
        "class SimpleTransformerWithDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
        "        super(SimpleTransformerWithDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src) * torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
        "        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, d_model)\n",
        "        tgt = tgt.permute(1, 0, 2)\n",
        "        output = self.transformer(src, tgt)\n",
        "        output = output.permute(1, 0, 2)  # Convert back to (batch_size, seq_len, d_model)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Define a simple dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoded_text = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return encoded_text['input_ids'].squeeze(), encoded_text['attention_mask'].squeeze()\n",
        "\n",
        "# Helper function to plot loss\n",
        "def plot_loss(losses):\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(losses)), losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Over Epochs')\n",
        "    st.pyplot(plt)\n",
        "    st.markdown(\"\"\"\n",
        "    **Training Loss Over Epochs**: This graph shows how the model's error decreases over time as it learns from the training data. A lower loss indicates a better performing model.\n",
        "    \"\"\")\n",
        "\n",
        "# Helper function for model evaluation\n",
        "def evaluate(model, tokenizer, text, max_length=10):\n",
        "    model.eval()\n",
        "    encoded_text = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids = encoded_text['input_ids']\n",
        "    tgt_input = input_ids[:, :-1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, tgt_input)\n",
        "\n",
        "    output_tokens = output.argmax(dim=-1)\n",
        "    decoded_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "    return decoded_output\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Build a Transformer Model (LLM) with PyTorch\")\n",
        "\n",
        "# Step 1: Define model parameters\n",
        "st.header(\"Step 1: Define Model Parameters\")\n",
        "st.markdown(\"In this step, you will define the parameters for your transformer model. These parameters determine how the model processes and learns from the data.\")\n",
        "\n",
        "# Add explainer for model parameters\n",
        "d_model_explainer = \"A higher number can capture more complex patterns but requires more computational power.\"\n",
        "nhead_explainer = \"More heads can help the model learn more complex relationships.\"\n",
        "num_encoder_layers_explainer = \"More layers can improve understanding but require more computation.\"\n",
        "num_decoder_layers_explainer = \"More layers can help generate more accurate results but require more computation.\"\n",
        "\n",
        "# Update slider names to be more descriptive and add explanations\n",
        "d_model = st.slider(\"Embedding Size (d_model): Number of features in each word's vector representation.\", 128, 512, 256, step=64, help=d_model_explainer)\n",
        "nhead = st.slider(\"Number of Attention Heads (nhead): How many parts of the data to focus on at once.\", 1, 8, 4, help=nhead_explainer)\n",
        "num_encoder_layers = st.slider(\"Number of Encoder Layers: How many layers the model uses to process and understand the input data.\", 1, 6, 2, help=num_encoder_layers_explainer)\n",
        "num_decoder_layers = st.slider(\"Number of Decoder Layers: How many layers to generate the output.\", 1, 6, 2, help=num_decoder_layers_explainer)\n",
        "\n",
        "vocab_size = 30522  # Using BERT base uncased tokenizer's vocab size\n",
        "\n",
        "if st.button(\"Initialize Model\"):\n",
        "    if d_model % nhead == 0:\n",
        "        model = SimpleTransformerWithDecoder(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "        st.session_state.model = model\n",
        "        st.success(\"Model initialized!\")\n",
        "    else:\n",
        "        st.error(\"Error: Embedding Size (d_model) must be divisible by Number of Attention Heads (nhead)\")\n",
        "\n",
        "# Step 2: Load and preprocess data\n",
        "st.header(\"Step 2: Load and Preprocess Data\")\n",
        "st.markdown(\"In this step, you will load and preprocess the data that the model will be trained on. Preprocessing ensures the data is in the right format for the model.\")\n",
        "\n",
        "texts = [\"I love programming.\", \"Python is great.\", \"Transformers are powerful.\"]\n",
        "try:\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    st.session_state.tokenizer = tokenizer\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading tokenizer: {e}\")\n",
        "\n",
        "if 'tokenizer' in st.session_state:\n",
        "    tokenizer = st.session_state.tokenizer\n",
        "    dataset = TextDataset(texts, tokenizer, max_length=10)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "    st.write(\"**Sample Texts:** These are the example sentences that will be used for training and evaluation.\")\n",
        "    st.write(texts)\n",
        "    st.write(\"**Tokenized Sample:** This shows the numerical representation of the sample texts after tokenization, which is necessary for the model to process the data.\")\n",
        "    sample_data = next(iter(dataloader))\n",
        "    st.write(sample_data)\n",
        "\n",
        "    if st.button(\"Preprocess Data\"):\n",
        "        st.session_state.dataloader = dataloader\n",
        "        st.success(\"Data preprocessed!\")\n",
        "\n",
        "# Step 3: Train the model\n",
        "st.header(\"Step 3: Train the Model\")\n",
        "st.markdown(\"In this step, you will train the transformer model using the preprocessed data. Training involves adjusting the model's parameters to minimize the error on the training data.\")\n",
        "\n",
        "num_epochs = st.slider(\"Number of Epochs: Times the training algorithm will pass through the entire training dataset.\", 1, 20, 10, help=\"More epochs can improve model performance but might also lead to overfitting.\")\n",
        "learning_rate = st.slider(\"Learning Rate: How quickly the model updates its parameters.\", 0.0001, 0.01, 0.001, step=0.0001, help=\"A higher learning rate can speed up training but might overshoot the optimal solution.\")\n",
        "\n",
        "if \"model\" in st.session_state and \"dataloader\" in st.session_state:\n",
        "    model = st.session_state.model\n",
        "    dataloader = st.session_state.dataloader\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if st.button(\"Start Training\"):\n",
        "        losses = []\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_loss = 0\n",
        "            for src, _ in dataloader:\n",
        "                tgt_input = src[:, :-1]\n",
        "                tgt_output = src[:, 1:]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(src, tgt_input)\n",
        "                loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            average_loss = epoch_loss / len(dataloader)\n",
        "            losses.append(average_loss)\n",
        "            st.write(f\"Epoch {epoch+1}, Loss: {average_loss}\")\n",
        "\n",
        "        st.session_state.losses = losses\n",
        "        st.success(\"Training completed!\")\n",
        "        plot_loss(losses)\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "st.header(\"Step 4: Evaluate the Model\")\n",
        "st.markdown(\"In this step, you can evaluate the trained model by inputting text and observing the generated output. This helps you understand how well the model has learned from the training data.\")\n",
        "\n",
        "input_text = st.text_input(\"Enter text for evaluation\", \"I love\")\n",
        "\n",
        "if \"model\" in st.session_state and st.button(\"Evaluate\"):\n",
        "    model = st.session_state.model\n",
        "    tokenizer = st.session_state.tokenizer\n",
        "    predicted_text = evaluate(model, tokenizer, input_text)\n",
        "    st.write(f\"Input Text: {input_text}\")\n",
        "    st.write(f\"Predicted Continuation: {predicted_text}\")\n",
        "\n",
        "# Add download button\n",
        "if \"model\" in st.session_state:\n",
        "    torch.save(st.session_state.model.state_dict(), \"model.pth\")\n",
        "    with open(\"model.pth\", \"rb\") as file:\n",
        "        st.download_button(\n",
        "            label=\"Download Model\",\n",
        "            data=file,\n",
        "            file_name=\"model.pth\",\n",
        "            mime=\"application/octet-stream\"\n",
        "            )\n",
        "\n",
        "# Additional functionalities: by Hugging Face Pipeline\n",
        "st.header(\"Additional Functionalities: Hugging Face Pipeline\")\n",
        "st.markdown(\"These additional functionalities utilize pre-trained models by Hugging Face to perform tasks. These tools can help enhance the performance and capabilities of your model.\")\n",
        "\n",
        "# Masked Language Modeling\n",
        "st.subheader(\"Masked Language Modeling\")\n",
        "st.markdown(\"**Masked Language Modeling**: This technique involves masking a word in a sentence and having the model predict the masked word. It's important for understanding context and improving the model's ability to fill in missing information.\")\n",
        "masked_text = st.text_input(\"Enter text with [MASK] token\", \"Hello I'm a [MASK] model.\")\n",
        "if st.button(\"Fill Mask\"):\n",
        "    unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "    results = unmasker(masked_text)\n",
        "    st.write(results)\n",
        "\n",
        "# Feature Extraction\n",
        "st.subheader(\"Feature Extraction\")\n",
        "st.markdown(\"**Feature Extraction**: This process involves converting raw text into numerical features that can be used by machine learning models. It's important because it allows the model to process and understand the input text.\")\n",
        "feature_text = st.text_input(\"Enter text for feature extraction\", \"Replace me by any text you'd like.\")\n",
        "if st.button(\"Extract Features\"):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    encoded_input = tokenizer(feature_text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded_input)\n",
        "    st.write(output)\n",
        "\n",
        "# Additional information\n",
        "st.header(\"Further Learning Resources\")\n",
        "st.markdown(\"\"\"\n",
        "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
        "- [Hugging Face Transformers Documentation](https://huggingface.co/transformers/)\n",
        "- [\"Attention Is All You Need\" Paper](https://arxiv.org/abs/1706.03762)\n",
        "- [Deep Learning Specialization by Andrew Ng (Coursera)](https://www.coursera.org/specializations/deep-learning)\n",
        "- [Natural Language Processing with Deep Learning (Stanford)](http://web.stanford.edu/class/cs224n/)\n",
        "- [Hugging Face's \"Transformers\" Course](https://huggingface.co/course/chapter1)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "mbFBXXcppkaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd788e6-5ef5-4402-e4a2-da97108b41b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.32.154.232"
          ]
        }
      ],
      "source": [
        "!npx curl https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUifRTKqplNQ",
        "outputId": "81477306-c458-46ba-e90a-aada96164dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.013s\n",
            "your url is: https://whole-toes-float.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN65C1tsv1cJ0sJ3BNdDGnN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}